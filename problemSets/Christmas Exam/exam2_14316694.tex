\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}


\title{Exam 2 (Christmas)}
\date{Due: December 08, 2021}
\author{Applied Stats/Quant Methods 1}

\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
		\item Instructions
		Please read carefully: You have until 23:59 Wednesday December 8 to complete
		the exam. Please export your answers as a single PDF file and include all code you
		produce in a supporting R file, which you will upload to Blackboard. The exam is
		open book; you can consult any materials you like. You must not collborate with
		or seek help from other students. In case of questions or technical difficulties, you
		can contact me via email. You should write-up your answers in R and LaTeX as you
		would for a problem set. Please make sure to concisely number your answers so that
		they can be matched with the corresponding questions. 
		
		
	\end{itemize}
	\lstinputlisting[language=R, firstline=10, lastline=81]{exam2_14316694.pdf}
	
	\newpage
	
		\vspace{.5cm}
	\section*{Question 1: Stock Market DONE}
	Suppose we were interested in studying the value of our company in the stock market. Figure
	1 plots the total value of our stock (the y-axis is in dollars).
	
	\vspace{
	\noindent 	
	
	\noindent 
	
	\begin{enumerate}
		
		\item [(a)] What concerns might we have about using the value of our company ‘as is’ in a model that
		regresses 
		‘Total Stock Value’ on ‘Months After Purchase’?
		Skewness and Kurtosis. 
		
		This appears to be an example of data that are drawn from a distribution that is right-skewed or positive skewed (in this case it appears to be the exponential distribution). 
		
		
		Another concern for this data may be excess kurtosis, either positive or negative. Underdispersed data has negative excess kurtosis and therefore a reduced number of outliers. 
		Overdispersed data has a positive excess kurtosis and an increased number of outliers.
		
		\begin{verbatim}	
		\end{verbatim}
		
		\item [(b)]  How could we address these
		concerns?
		Skew: There are apparently numerous ways to address skew in data.One way is the data can be transformed, using log and root for exponential distributions and Box Cox for skewed distributions. Outliers can also be removed manually, identifying them using z scores and interquartile range.
		
	Kurtosis: Transforming the data. The Box-Cox transformation is a useful technique for trying to normalise a data set. Another technique available is the probability plot correlation coefficient plot and the probability plot to investigate a good distributional model for the data. 
	
	\end{enumerate}
	
	\newpage
	
	\section*{Question 2: Lambs DONE}
	This data set presents information on 33 lambs, of which 11 are ewe lambs, 11 are wether
	lambs, and 11 are ram lambs. These lambs grazed together in the same pasture and were
	treated similarly in all ways. The variables of interest for this question are stated below.
	The objective is to determine whether differences in Fatness could be attributed to Group
	while accounting for Weight. Information on the data and the model fit in R are given below:

	\vspace{.25cm}
	\noindent 
	
	
	\vspace{.5cm}
	
	
	\vspace{.5cm}
	\begin{enumerate}
		\item [(a)] Write out the fitted model for a ewe lamb using the estimated coefficients.
		Write the prediction equation based on the result.
		
		$\hat{Y_i}=\beta_0+\beta_1x_i+\beta_2x_iD_i$
		
		This is the equation or fitted regression line that applied in the last data set. However in this equation there is a second dummy variable
		
		$\hat{Y_i}=\beta_0+\beta_1x_i+\beta_2x_iD_1i$+\beta_3x_iD_2i$
		
		Predicted value of Fatness = -18.137 + 2.298*Weight -8.362*Wether:NotWether  -4.072*Ram:NotRam
		
		DONE
		\begin{verbatim}
			
		\end{verbatim}
		
		\newpage		
		\item [(b)] What is the predicted Fatness index of a wether lamb that weighs 14kg?
		
		\begin{verbatim}
		Predicted value of Fatness = -18.137 + 2.298*14 -8.362*1 -4.072*0
		
		Predicted value of Fatness = -18.137 + 32.172 -8.362
		
		 Predicted Fatness index = 5.673
		 
		 DONE
		\end{verbatim}
		
		
		\item [(c)] Which lamb group has the highest Fatness index for every weight?
			
	Perhaps need to perform a test in Rstudio for this one?
	
	Predicted Fatness Index of Wether group = -18.137 -8.362*1 -4.072*0
	Predicted Fatness Index of Wether group = -18.137 -8.362
	Predicted Fatness Index of Wether group = -26.499
	
	Predicted Fatness Index of Ram group = -18.137 -8.362*0 -4.072*1
	Predicted Fatness Index of Ram group = -18.137 -4.072*1
	Predicted Fatness Index of Ram group = -18.137 -4.072
	Predicted Fatness Index of Ram group = -22.209
	
	
	Predicted Fatness Index of Ewe group = -18.137 -8.362*0 -4.072*0
	Predicted Fatness Index of Ewe group = -18.137
	
	They are all negative values, so it does not make sense? If I ignore the negative values, then the Wether group has the highest Fatness index for every weight.
	
	DONE- ISH
	
		\begin{verbatim}
			
			
		\end{verbatim}
		\vspace{.5cm}
	\section*{Question 3: Arsenic}  
		\item [(a)] 
		So, we successfully estimated an additive model with well depth and distance to the
		nearest factory as the two predictors of a household’s arsenic level. The estimated
		coefficients are found in the first column of Table 1. Interpret the estimated coefficients
		for the intercept and each predictor.
		
		
		\begin{verbatim}
		
			
			
		\end{verbatim}
		\item [(b)] Does the coefficient estimate for the closest known factory vary based on whether or not
		a house has a deep well? If so, change your interpretation of the estimated coefficients
		in part (a) to conform with the interactive model in column 2 of Table 1. Provide
		the appropriate test to determine whether we should model the relationship between
		distance, well depth, and arsenic levels using an additive or interactive model.

		
		\begin{verbatim}
			
			
			
		\end{verbatim}
	\item [(c)] Compute the average difference in arsenic levels between two households that have a
	deep well (=1), but one is closer to a factory (dist100 = 0.4) than the other (dist100 =
	2.14).
	
	\begin{verbatim}
		
		
		
	\end{verbatim}
\newpage
\section*{Question 4:Multiple Choice DONE}
\item [(a)] For explanatory variables with multi-collinearity, the corresponding estimated slopes have
larger standard errors.
VIFDONE .

\item [(b)] The coefficients in an ordinary least squares regression model are generalized additive estimates.
DONE 

\item [(c)] We can calculate our standard errors by taking the square root of the off-diagonal elements
in our variance-covariance matrix.
FALSE
 diagonal elements DONE

\item [(d)] Which of the following plots is used to check for normality in the assumptions of linear
regression?
The QQ plot of residuals.
DONE
 
	\vspace{.5cm}
	\newpage
\section*{Question 5: Climate Action HALF OF B AND ALL OF C LEFT}
\item [(a)]  Interpret the coefficients for Age and Education
Here those who are 70 and above have 8.413 less feeling thermometer ratings units. 

For every year more the respondents indicated they attended school for Education, 7.987 less feeling thermometer rating units for support for climate action were applicable to them. 

DONE

\item [(b)] ) The author claims that she ’cannot reject the null hypothesis that Age has no effect
on support for climate action (H0 : βAge = 0) . Using the coefficient estimate and the
standard error for Age construct a 95% confidence interval for the effect of Age on
support for climate action. Based on the confidence interval, do you agree with the
author? Explain your answer.


	The 95
	percent confidence interval for the slope is the estimated coefficient (-8.413) ± two standard errors (4.539*2).
	
	-8.413-9.078
	-8.413+9.078
	
	-17.491 \ge x \le 0.665
	
	\begin{verbatim}
	Using R: 
	confint(fit, 'Age', level= 0.95)
\end{verbatim}

BASED ON YOUR ANSWER, DO YOU AGREE WITH THE AUTHOR?

DONE
\item [(c)] Calculate the first difference in support for climate action between low and high values
of Education for young respondents holding Party constant at its sample mean. Use
3.93 as the mean of Party and use +/- one standard deviation around the mean of
Education (from 10.99 to 12.99) for low and high values of Education respectively


Predicted support for climate action =  
\begin{verbatim}
	
	
	
\end{verbatim}
	\vspace{.5cm}
	\newpage
\section*{Question 6: Define importance of terms DONE}
Define and describe why the following four (4) terms are important to hypothesis testing
and/or regression. You can earn full credit with just two or three sentences, but please be
specific and thorough.
\item [(a)] Partial F-test
Used to determine whether there is a statistically significant difference between a regression model and a subset of variables/ nest version of the overall regression model. Used to test the usefulness of a group of specific predictors in the overall model i.e. improve the fit of the model.
The null hypothesis is that all coefficients removed from the overall model are zero. The alternative is that at least one of these removed coefficients in the nest model is not zero. DONE

\begin{verbatim}
\end{verbatim}
\item [(b)] Categorical data/dummy variables
 It does not make sense to assign values of 1,2,3, to categorical data. Decomposing categorical variables into dummy variables in this case is important, assigning values of instead 1 or 0. This allows us to use Categorical data to predict in our regression.
Dummy variable models apparently are also important provide correct results for un/imbalanced data. DONE 
\begin{verbatim}
\end{verbatim}
\item [(c)] Constituent term, alse referred to as constitutive terms, is used in relation to the interaction effect. Example of constituent or constitutive terms is the/a coefficient. The product of constitutive terms is interaction terms.
DONE
\begin{verbatim}
\end{verbatim}
\item [(d)] The Test statistic is important as it let's us know whether we can reject or fail to reject the null hypothesis test. If the standardised test statistic is more extreme than the critical value, reject. If not more extreme, fail to reject. For example, a two sample t-test  tests if the means of two populations are equal. DONE
\begin{verbatim}
\end{verbatim}
	\end{enumerate}  
	
	
\end{document}
	

	
	
